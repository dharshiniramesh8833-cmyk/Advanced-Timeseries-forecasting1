This project presents a complete workflow for advanced multivariate time-series forecasting using deep learning techniques. The dataset used in this study consists of 1200 observations across 8 independent features, each demonstrating complex temporal dynamics, including long-term seasonality, short-period oscillations, nonlinear fluctuations, additive trends, and random noise. These characteristics make the dataset highly representative of real-world forecasting challenges found in fields such as finance, climate modeling, energy consumption, IoT sensor behavior, and industrial systems.

To effectively model these patterns, two neural-based forecasting models were developed and compared: a Transformer encoder model and a baseline LSTM model. The Transformer model leverages multi-head self-attention to capture long-range dependencies across time, allowing it to weigh the importance of different time steps dynamically. In contrast, the LSTM model represents a classical recurrent-based architecture designed to handle sequential data by retaining memory through gated mechanisms.

The dataset underwent several preprocessing steps to ensure optimal model performance. All features were normalized using a StandardScaler, and a sliding-window technique was implemented to convert the continuous time series into supervised learning samples. Specifically, a window size of 50 time steps was used, where the previous 50 values from all features served as input to predict the next time step. The dataset was then split into training and testing sets using an 80:20 ratio.

Training was conducted for 10 epochs for both models using the Adam optimizer and mean squared error (MSE) as the loss function. Throughout training, the Transformer consistently showed faster convergence and lower reconstruction error compared to the LSTM. After training, both models were evaluated using standard forecasting metrics such as MSE, RMSE, and MAE. The Transformer model achieved superior accuracy across all metrics, indicating a better ability to capture both short-term variations and longer temporal dependencies present in the data.

In addition to quantitative evaluation, several visualizations were produced to support model interpretation. These include line plots of the original time-series features, prediction-versus-actual graphs for both models, and temporal behavior illustrations highlighting the difference in model performance. The results clearly showed that the Transformer generated smoother and more accurate predictions, while the LSTM occasionally struggled with highly nonlinear components and rapid fluctuations.

Overall, this project successfully demonstrates the implementation of modern deep learning architectures for complex time-series forecasting. The comparative analysis highlights the advantages of attention-based models like Transformers over traditional recurrent models, especially in handling multivariate and long-range temporal dependencies. This work provides a strong foundation for future extensions such as multi-step forecasting, attention visualization, anomaly detection, or deployment in real-world applications.
